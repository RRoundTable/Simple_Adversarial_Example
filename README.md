# Simple_Adversarial_Example
Adversarial example입니다. 

## Adversarial Attack이란,

적은 변화로 학습시킨 모델의 판단의 혼란을 야기한다. 

특히, 인간의 판단에는 영향을 끼치지 않을 정도로 작은 변화이나 모델의 판단에는 큰 변화를 준다.

![example](https://blog.openai.com/content/images/2017/02/adversarial_img_1.png)

## Dataset

- cifar10

## Model

- VGG16

## result


